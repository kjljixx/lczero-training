%YAML 1.2
---
name: '128x10-744706-lr0.0001-5009games-feature-extractor'                       # ideally no spaces
gpu: 0                                 # gpu id to process on

dataset: 
  num_chunks: 76                 # newest nof chunks to parse
  train_ratio: 0.90                    # trainingset ratio
  # For separated test and train data.
  # input_train: '/path/to/chunks/*/draw/' # supports glob
  # input_test: '/path/to/chunks/*/draw/'  # supports glob
  # For a one-shot run with all data in one directory.
  input: '/mnt/c/Users/kjlji/OneDrive/Documents/VSCode/trainingdata-tool/supervised-0/'

training:
    batch_size: 100                   # training batch
    test_steps: 2000                   # eval test set values after this many steps
    train_avg_report_steps: 10        # training reports its average values after this many steps.
    total_steps: 80000                # terminate after these steps
    checkpoint_steps: 10000          # frequency for checkpointing
    warmup_steps: 250                  # if global step is less than this, scale the current LR by ratio of global step to this value
    # checkpoint_steps: 10000          # optional frequency for checkpointing before finish
    shuffle_size: 100               # size of the shuffle buffer
    lr_values:                         # list of learning rates
        - 0.0001
        - 0.00001
        - 0.000005
    lr_boundaries:                     # list of boundaries
        - 50000
        - 65000
    policy_loss_weight: 1.0            # weight of policy loss
    value_loss_weight: 1.0             # weight of value loss
    moves_left_loss_weight: 1.0        # weight of moves-left loss
    path: '/networks'    # network storage dir

model:
  # filters: 128
  filters: 128
  residual_blocks: 10
  se_ratio: 1                          # Squeeze Excite structural network architecture.
  policy: 'convolution'                  # attention policy fields:
  pol_embedding_size: 64               # embedding vector size
  pol_encoder_layers: 1                # number of intermediate attention layers in the policy head
  pol_encoder_heads: 4                 # number of attention heads in encoder layers
  pol_encoder_d_model: 64              # size of the Q, K, & V vectors in encoder layers -- divisible by encoder_heads
  pol_encoder_dff: 128                 # size of the largest dense layer in encoder block feed-forward network
  policy_d_model: 64                   # size of the query and key vectors in final attention layer
  value: 'wdl'
  moves_left: 'v1'
...
